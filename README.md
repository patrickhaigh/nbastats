BREIF OVERVIEW

This project tracks NBA player statistics using automated daily data collection from Basketball Reference. First, the main dataset focuses on team totals for the 2025-26 season. The data updates daily at 8am during the season after games are completed. Each daily scrape creates a new raw CSV file with the current date. Therefore, the project stores a full history of how the players individual statistics change over time. The cleaned data is stored in a combined file that is updated automatically.
This information is useful for fans, analysts, and anyone interested (like myself) in tracking player performance across the season. Then, users can see trends in points, rebounds, field goal percentage, or any stat of their interest. The tools from this class are used to automate the full workflow, and R scripts handle scraping, cleaning, and visualization. GitHub Actions runs the pipeline on a schedule so the data updates automatically which turns a dataset into a live, self updating project.



WRITTEN SUMMARY AND REFLECTION

The primary data source for this project is Basketball Reference. First, the project scrapes individual NBA player statistics from the live season totals page once per day. Each run of the scraper creates a dated raw file in the data/raw folder. Therefore, this structure preserves historical snapshots of how player performance changes throughout the season instead of overwriting past values. The player level data includes core performance measures such as games played, minutes, points, rebounds, assists, and other box score totals. Because these values update after every slate of games, the dataset changes daily and reflects real time player production.

  The automated pipeline is controlled through GitHub Actions using the nba_totals.yml workflow file. First, the workflow initializes the R environment. Next, it runs the scraping script to pull updated player statistics from Basketball Reference. Then, it runs the cleaning script to merge and standardize all raw files into a single structured dataset. After that, it runs the dashboard update script, which regenerates the visualization using the most recent clean data. Finally, the workflow commits and pushes all updated data and figures back into the repository automatically. As a result, the entire project updates without any manual input once it is deployed.

  The data cleaning process focuses on consistency, usability, and long term tracking of player performance. First, player names and team abbreviations are standardized so that players can be tracked correctly across multiple days and team changes. Then, numeric columns are converted into proper data types so calculations and plots function correctly. However, repeated daily scrapes naturally introduce repeated rows across dates. These are handled by appending each day as a new observation labeled with a scrape date rather than collapsing the data. Therefore, each row in the clean dataset represents one player on one specific day of the season. This structure allows for time based analysis of improvement, decline, and consistency.

  The main visualization in this project highlights the top players based on total points using an automatically updated plot stored in the figs folder. First, the chart displays the player names at the bottom. Then, it provides a line graph tracking the highest point totals by player. Think of it as tracking a horserace as the season goes on, players constantly battling for position. Therefore, even someone with no experience in data analysis can interpret who the top scorers are by simply viewing the graphic. Because the visualization updates whenever the workflow runs, users always see the most current points leader without needing to run any code themselves.
    
This project is designed so that a user with no technical background can still understand the results. First, the repository is organized so that raw data, clean data, scripts, and figures are stored in separate folders. Then, the README explains the process on the home page of the repository. Therefore, someone can view the figures, read the summary and inspect my folders without needing to understand how the R codes or yml actions work.
  
The most challenging part of this project was making sure that all three R scripts functioned correctly before running them automatically through GitHub. I had to first get each script to work on its own when run in R. However, running them through GitHub Actions introduced new issues with file paths, package loading, and execution order. Therefore, I had to look up how to get my output into each file correctly to make it accessible to the reader. There were multiple times a single error in one script caused the entire pipeline to fail, making the trial and error process more frustrating.
  
The most important lesson learned from this project was that data collection can be automated. It can be a tedious task to manually code whenever you want to scrape information from the web. This can be even more tedious when the information you'd like to scrape updated daily, for example, sports statistics, movie rankings, etc. With a Github repository, data wranggling can be simple and automated as you only have to perform the scrapping and cleaning a single time to get your desired outcome.
